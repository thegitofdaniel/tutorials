{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel Rocha Ruiz, MSc in Data Science and Business Analytics\n",
    "\n",
    "Sources:\n",
    "- http://archive.ics.uci.edu/ml/machine-learning-databases/00374/\n",
    "- http://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction\n",
    "- https://databricks.com/blog/2019/09/10/doing-multivariate-time-series-forecasting-with-recurrent-neural-networks.html\n",
    "- https://pages.databricks.com/rs/094-YMS-629/images/Blog_%20A%20Multivariate%20Time%20Series%20Forecasting%20Appliance%20Energy%20Usage.html\n",
    "- https://pages.databricks.com/rs/094-YMS-629/images/Blog_%20A%20Multivariate%20Time%20Series%20Forecasting%20Appliance%20Energy%20Usage.html\n",
    "- https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "# Set-up\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from time import time\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# mlflow\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# keras\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import Dense, CuDNNLSTM, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'dbutils' from 'tensorflow.keras' (C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dbutils\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'dbutils' from 'tensorflow.keras' (C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import dbutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ", dbutils, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Using cached mlflow-1.25.1-py3-none-any.whl (16.8 MB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (21.3)\n",
      "Requirement already satisfied: protobuf>=3.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (3.20.1)\n",
      "Requirement already satisfied: entrypoints in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (0.4)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (1.22.3)\n",
      "Requirement already satisfied: sqlparse>=0.3.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (0.4.2)\n",
      "Requirement already satisfied: pytz in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (2022.1)\n",
      "Collecting docker>=4.0.0\n",
      "  Using cached docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Collecting Flask\n",
      "  Using cached Flask-2.1.2-py3-none-any.whl (95 kB)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Using cached GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Collecting cloudpickle\n",
      "  Using cached cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (1.4.2)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,>=3.7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (4.11.3)\n",
      "Requirement already satisfied: requests>=2.17.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (2.27.1)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Using cached databricks_cli-0.16.6-py3-none-any.whl\n",
      "Collecting alembic\n",
      "  Using cached alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (1.8.0)\n",
      "Collecting waitress\n",
      "  Using cached waitress-2.1.1-py3-none-any.whl (57 kB)\n",
      "Collecting querystring-parser\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Using cached prometheus_flask_exporter-0.20.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: click>=7.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (8.1.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp39-cp39-win_amd64.whl (151 kB)\n",
      "Requirement already satisfied: sqlalchemy in c:\\programdata\\miniconda3\\lib\\site-packages (from mlflow) (1.4.36)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from click>=7.0->mlflow) (0.4.4)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Using cached PyJWT-2.3.0-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow) (1.16.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in c:\\programdata\\miniconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow) (0.8.9)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from databricks-cli>=0.8.7->mlflow) (3.2.0)\n",
      "Collecting pywin32==227\n",
      "  Using cached pywin32-227-cp39-cp39-win_amd64.whl (9.1 MB)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from docker>=4.0.0->mlflow) (1.3.2)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from importlib-metadata!=4.7.0,>=3.7.0->mlflow) (3.8.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\miniconda3\\lib\\site-packages (from requests>=2.17.3->mlflow) (3.3)\n",
      "Collecting Mako\n",
      "  Using cached Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\miniconda3\\lib\\site-packages (from sqlalchemy->mlflow) (1.1.2)\n",
      "Requirement already satisfied: Werkzeug>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from Flask->mlflow) (2.1.2)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from Flask->mlflow) (2.1.2)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from Flask->mlflow) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\miniconda3\\lib\\site-packages (from Jinja2>=3.0->Flask->mlflow) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\miniconda3\\lib\\site-packages (from packaging->mlflow) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from pandas->mlflow) (2.8.2)\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
      "Installing collected packages: pywin32, pyjwt, prometheus-client, Mako, gitdb, Flask, waitress, querystring-parser, pyyaml, prometheus-flask-exporter, gitpython, docker, databricks-cli, cloudpickle, alembic, mlflow\n",
      "Successfully installed Flask-2.1.2 Mako-1.2.0 alembic-1.7.7 cloudpickle-2.0.0 databricks-cli-0.16.6 docker-5.0.3 gitdb-4.0.9 gitpython-3.1.27 mlflow-1.25.1 prometheus-client-0.14.1 prometheus-flask-exporter-0.20.1 pyjwt-2.3.0 pywin32-227 pyyaml-6.0 querystring-parser-1.2.4 waitress-2.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script mako-render.exe is installed in 'C:\\Users\\drr19\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script flask.exe is installed in 'C:\\Users\\drr19\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script waitress-serve.exe is installed in 'C:\\Users\\drr19\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts databricks.exe and dbfs.exe are installed in 'C:\\Users\\drr19\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'C:\\Users\\drr19\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script mlflow.exe is installed in 'C:\\Users\\drr19\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install mlflow --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1583149932.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [22]\u001b[1;36m\u001b[0m\n\u001b[1;33m    os.environ['PYSPARK_SUBMIT_ARGS'] = \"--master mymaster --total-executor 2 --conf \"spark.driver.extraJavaOptions=-Dhttp.proxyHost=proxy.mycorp.com-Dhttp.proxyPort=1234 -Dhttp.nonProxyHosts=localhost|.mycorp.com|127.0.0.1 -Dhttps.proxyHost=proxy.mycorp.com -Dhttps.proxyPort=1234 -Dhttps.nonProxyHosts=localhost|.mycorp.com|127.0.0.1 pyspark-shell\"\u001b[0m\n\u001b[1;37m                                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = \"jupyter\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPython Spark SQL basic example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.some.config.option\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msome-value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pyspark\\context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 392\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pyspark\\context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 339\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    111\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Python Spark SQL basic example\") \\\n",
    "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/energydata_complete.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls and date column\n",
    "df = df.dropna() \n",
    "dataset = df.drop('date')\n",
    "\n",
    "# Round up the columns data to one decimal point\n",
    "columns = dataset.columns\n",
    "for i in columns:\n",
    "    dataset = dataset.withColumn(i, round(i, 1))\n",
    "    \n",
    "values = dataset.collect()\n",
    "\n",
    "\n",
    "\n",
    "values = dataset.collect() # should return a list \n",
    "\n",
    "# standardize\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "X = scaled[:][:-144]\n",
    "y = scaled[:, 0][144:]\n",
    "\n",
    "# split into train and test sets\n",
    "trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.20, random_state=42, shuffle = False)\n",
    "\n",
    "# Create overlapping windows of lagged values for training and testing datasets\n",
    "timesteps = 864\n",
    "train_generator = TimeseriesGenerator(trainX, trainY, length=timesteps, sampling_rate=1, batch_size=timesteps)\n",
    "test_generator = TimeseriesGenerator(testX, testY, length=timesteps, sampling_rate=1, batch_size=timesteps)\n",
    "\n",
    "train_X, train_y = train_generator[0]\n",
    "test_X, test_y = test_generator[0]\n",
    "\n",
    "train_samples = train_X.shape[0]*len(train_generator)\n",
    "test_samples = test_X.shape[0]*len(test_generator)\n",
    "\n",
    "print(\"Total Records (n): {}\".format(df.count()))\n",
    "print(\"Total Records after adjusting for 24 hours: {}\".format(len(X)))\n",
    "print(\"Number of samples in training set (.8 * n): trainX = {}\".format(trainX.shape[0]))\n",
    "print(\"Number of samples in testing set (.2 * n): testX = {}\".format(testX.shape[0]))\n",
    "print(\"Size of individual batches: {}\".format(test_X.shape[1]))\n",
    "print(\"Number of total samples in training feature set: {}\".format(train_samples))\n",
    "print(\"Number of samples in testing feature set: {}\".format(test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "tb_dir = '/tmp/tensorflow_log_dir/{}'.format(time())\n",
    "tensorboard = TensorBoard(log_dir = tb_dir)\n",
    "dbutils.tensorboard.start(tb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM expects the input data in a specific 3D format of test sample size, time steps, no. of input features. We had defined the time steps as n_lag variable in previous step.  Time steps are the past observations that the network will learn from (e.g. backpropagation through time).\n",
    "# For details on what individual hyperparameters mean, see here: https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L2051\n",
    "\n",
    "units = 128\n",
    "num_epoch = 5000\n",
    "learning_rate = 0.00144\n",
    "\n",
    "with mlflow.start_run(experiment_id=3133492, nested=True):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(units, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(LeakyReLU(alpha=0.5)) \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    # Stop training when a monitored quantity has stopped improving.\n",
    "    callback = [EarlyStopping(monitor=\"loss\", min_delta = 0.00001, patience = 50, mode = 'auto', restore_best_weights=True), tensorboard] \n",
    "\n",
    "    # Using regression loss function 'Mean Standard Error' and validation metric 'Mean Absolute Error'\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit_generator(train_generator, \\\n",
    "                                epochs=num_epoch, \\\n",
    "                                validation_data=test_generator, \\\n",
    "                                callbacks = callback, \\\n",
    "                                verbose=2, \\\n",
    "                                shuffle=False, \\\n",
    "                                initial_epoch=0)\n",
    "\n",
    "    mlflow.log_param(\"Units\", units)\n",
    "    mlflow.log_param(\"Epochs\", num_epoch)\n",
    "    mlflow.log_param(\"Learning Rate\", learning_rate)\n",
    "    mlflow.log_param(\"Lags cosidered\", timesteps)\n",
    "\n",
    "    #   Return loss value and metric value\n",
    "    score = model.evaluate_generator(test_generator, verbose=0)   \n",
    "    mlflow.log_metric(\"Test Loss\", score[0]) \n",
    "    mlflow.log_metric(\"MAE\", score[1])   \n",
    "    mlflow.log_metric(\"Actual Epochs\", len(history.history['loss']))\n",
    "    mlflow.keras.log_model(model, \"LSTM Model\")\n",
    "\n",
    "    # The model can be saved for future use and move to production\n",
    "    #   mlflow.keras.save_model(model1, \"/dbfs/ved-demo/timeseries/best-appliance-model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series: Loss\n",
    "train_loss = np.mean(history.history['loss'])\n",
    "train_mae = np.mean(history.history['mean_absolute_error'])\n",
    "title = 'Train Loss: {0:.3f} Test Loss: {1:.3f}\\n  Train MAE: {2:.3f}, Val MAE: {3:.3f}'.format(train_loss, score[0], train_mae, score[1])\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.style.use('seaborn')\n",
    "# plot series\n",
    "plt.plot(history.history['loss'], 'c-', label='train')\n",
    "plt.plot(history.history['val_loss'], 'm:', label='test')\n",
    "# others\n",
    "plt.title(title)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.style.use('seaborn')\n",
    "palette = plt.get_cmap('Set1')\n",
    "# plot series\n",
    "plt.plot(y, marker='', color=palette(4), linewidth=1, alpha=0.9, label='actual')\n",
    "plt.plot(yhat_train_plot, marker='', color=palette(2), linewidth=1, alpha=0.9, label='training predictions')\n",
    "plt.plot(yhat_test_plot, marker='', color=palette(3), linewidth=1, alpha=0.9, label='testing predictions')\n",
    "# others\n",
    "plt.title('Appliances Energy Prediction', loc='center', fontsize=20, fontweight=5, color='orange')\n",
    "plt.ylabel('Energy used (Wh)')\n",
    "plt.legend()\n",
    "fig.set_size_inches(w=15,h=5)\n",
    "plt.close()\n",
    "display(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
