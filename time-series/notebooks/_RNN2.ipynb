{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniel Rocha Ruiz, MSc in Data Science and Business Analytics\n",
    "\n",
    "Sources:\n",
    "- http://archive.ics.uci.edu/ml/machine-learning-databases/00374/\n",
    "- http://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction\n",
    "- https://databricks.com/blog/2019/09/10/doing-multivariate-time-series-forecasting-with-recurrent-neural-networks.html\n",
    "- https://pages.databricks.com/rs/094-YMS-629/images/Blog_%20A%20Multivariate%20Time%20Series%20Forecasting%20Appliance%20Energy%20Usage.html\n",
    "- https://pages.databricks.com/rs/094-YMS-629/images/Blog_%20A%20Multivariate%20Time%20Series%20Forecasting%20Appliance%20Energy%20Usage.html\n",
    "- https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "\n",
    "# Set-up\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "from time import time\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# mlflow\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "# pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "# keras\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.layers import Dense, CuDNNLSTM, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import dbutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"Python Spark SQL basic example\") \\\n",
    "            .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/energydata_complete.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls and date column\n",
    "df = df.dropna() \n",
    "dataset = df.drop('date')\n",
    "\n",
    "# Round up the columns data to one decimal point\n",
    "columns = dataset.columns\n",
    "for i in columns:\n",
    "    dataset = dataset.withColumn(i, round(i, 1))\n",
    "    \n",
    "values = dataset.collect()\n",
    "\n",
    "\n",
    "\n",
    "values = dataset.collect() # should return a list \n",
    "\n",
    "# standardize\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(values)\n",
    "\n",
    "X = scaled[:][:-144]\n",
    "y = scaled[:, 0][144:]\n",
    "\n",
    "# split into train and test sets\n",
    "trainX, testX, trainY, testY = train_test_split(X, y, test_size=0.20, random_state=42, shuffle = False)\n",
    "\n",
    "# Create overlapping windows of lagged values for training and testing datasets\n",
    "timesteps = 864\n",
    "train_generator = TimeseriesGenerator(trainX, trainY, length=timesteps, sampling_rate=1, batch_size=timesteps)\n",
    "test_generator = TimeseriesGenerator(testX, testY, length=timesteps, sampling_rate=1, batch_size=timesteps)\n",
    "\n",
    "train_X, train_y = train_generator[0]\n",
    "test_X, test_y = test_generator[0]\n",
    "\n",
    "train_samples = train_X.shape[0]*len(train_generator)\n",
    "test_samples = test_X.shape[0]*len(test_generator)\n",
    "\n",
    "print(\"Total Records (n): {}\".format(df.count()))\n",
    "print(\"Total Records after adjusting for 24 hours: {}\".format(len(X)))\n",
    "print(\"Number of samples in training set (.8 * n): trainX = {}\".format(trainX.shape[0]))\n",
    "print(\"Number of samples in testing set (.2 * n): testX = {}\".format(testX.shape[0]))\n",
    "print(\"Size of individual batches: {}\".format(test_X.shape[1]))\n",
    "print(\"Number of total samples in training feature set: {}\".format(train_samples))\n",
    "print(\"Number of samples in testing feature set: {}\".format(test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard\n",
    "tb_dir = '/tmp/tensorflow_log_dir/{}'.format(time())\n",
    "tensorboard = TensorBoard(log_dir = tb_dir)\n",
    "dbutils.tensorboard.start(tb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM expects the input data in a specific 3D format of test sample size, time steps, no. of input features. We had defined the time steps as n_lag variable in previous step.  Time steps are the past observations that the network will learn from (e.g. backpropagation through time).\n",
    "# For details on what individual hyperparameters mean, see here: https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py#L2051\n",
    "\n",
    "units = 128\n",
    "num_epoch = 5000\n",
    "learning_rate = 0.00144\n",
    "\n",
    "with mlflow.start_run(experiment_id=3133492, nested=True):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(CuDNNLSTM(units, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(LeakyReLU(alpha=0.5)) \n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    # Stop training when a monitored quantity has stopped improving.\n",
    "    callback = [EarlyStopping(monitor=\"loss\", min_delta = 0.00001, patience = 50, mode = 'auto', restore_best_weights=True), tensorboard] \n",
    "\n",
    "    # Using regression loss function 'Mean Standard Error' and validation metric 'Mean Absolute Error'\n",
    "    model.compile(loss='mse', optimizer=adam, metrics=['mae'])\n",
    "\n",
    "    # fit network\n",
    "    history = model.fit_generator(train_generator, \\\n",
    "                                epochs=num_epoch, \\\n",
    "                                validation_data=test_generator, \\\n",
    "                                callbacks = callback, \\\n",
    "                                verbose=2, \\\n",
    "                                shuffle=False, \\\n",
    "                                initial_epoch=0)\n",
    "\n",
    "    mlflow.log_param(\"Units\", units)\n",
    "    mlflow.log_param(\"Epochs\", num_epoch)\n",
    "    mlflow.log_param(\"Learning Rate\", learning_rate)\n",
    "    mlflow.log_param(\"Lags cosidered\", timesteps)\n",
    "\n",
    "    #   Return loss value and metric value\n",
    "    score = model.evaluate_generator(test_generator, verbose=0)   \n",
    "    mlflow.log_metric(\"Test Loss\", score[0]) \n",
    "    mlflow.log_metric(\"MAE\", score[1])   \n",
    "    mlflow.log_metric(\"Actual Epochs\", len(history.history['loss']))\n",
    "    mlflow.keras.log_model(model, \"LSTM Model\")\n",
    "\n",
    "    # The model can be saved for future use and move to production\n",
    "    #   mlflow.keras.save_model(model1, \"/dbfs/ved-demo/timeseries/best-appliance-model/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series: Loss\n",
    "train_loss = np.mean(history.history['loss'])\n",
    "train_mae = np.mean(history.history['mean_absolute_error'])\n",
    "title = 'Train Loss: {0:.3f} Test Loss: {1:.3f}\\n  Train MAE: {2:.3f}, Val MAE: {3:.3f}'.format(train_loss, score[0], train_mae, score[1])\n",
    "\n",
    "# create plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.style.use('seaborn')\n",
    "# plot series\n",
    "plt.plot(history.history['loss'], 'c-', label='train')\n",
    "plt.plot(history.history['val_loss'], 'm:', label='test')\n",
    "# others\n",
    "plt.title(title)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.close()\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.style.use('seaborn')\n",
    "palette = plt.get_cmap('Set1')\n",
    "# plot series\n",
    "plt.plot(y, marker='', color=palette(4), linewidth=1, alpha=0.9, label='actual')\n",
    "plt.plot(yhat_train_plot, marker='', color=palette(2), linewidth=1, alpha=0.9, label='training predictions')\n",
    "plt.plot(yhat_test_plot, marker='', color=palette(3), linewidth=1, alpha=0.9, label='testing predictions')\n",
    "# others\n",
    "plt.title('Appliances Energy Prediction', loc='center', fontsize=20, fontweight=5, color='orange')\n",
    "plt.ylabel('Energy used (Wh)')\n",
    "plt.legend()\n",
    "fig.set_size_inches(w=15,h=5)\n",
    "plt.close()\n",
    "display(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
